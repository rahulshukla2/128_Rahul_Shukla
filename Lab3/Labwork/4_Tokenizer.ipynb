{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrZevCPJ92HG"
      },
      "source": [
        "import csv\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rmYBjsyCv3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70158c28-7b4c-4919-cf6f-cec5fa4ac8be"
      },
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "with open(\"bbc-text-small.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        sentence = row[1]\n",
        "        for word in stopwords:\n",
        "            token = \" \" + word + \" \"\n",
        "            sentence = sentence.replace(token, \" \")\n",
        "            sentence = sentence.replace(\"  \", \" \")\n",
        "        sentences.append(sentence)\n",
        "\n",
        "\n",
        "print(len(sentences))\n",
        "print(sentences[48])\n",
        "print(labels[48])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "libya takes $1bn unfrozen funds libya withdrawn $1bn assets us assets previously frozen almost 20 years libyan central bank said. move came us lifted trade ban reward tripoli giving weapons mass destruction vowing compensate lockerbie victims. original size libya s funds $400m central bank told reuters. however withdrawal not mean libya cut ties us added. process opening accounts banks united states central bank s vice president farhat omar ben gadaravice said. previously frozen assets invested various countries believed included equity holdings banks. us ban trade economic activity tripoli - imposed president ronald regan 1986 series us deemed terrorist acts including 1988 lockerbie air crash - suspended april. bankers two country s working unfreeze libya s assets.\n",
            "business\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LhzBBgSC3S5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddf75eb-2f9f-4d96-ee67-569aa8e5185c"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token= \"<OOV>\")\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'ml': 2, 'are': 3, 'subject': 4, 'prof': 5, 'at': 6, 'ddu': 7, 'we': 8, 'learning': 9, 'in': 10, 'semester': 11, '7': 12, 'i': 13, 'love': 14, 'brijesh': 15, 'bhatt': 16, 'and': 17, 'hariom': 18, 'pandya': 19, 'faculties': 20, 'for': 21, 'the': 22, 'do': 23, 'you': 24, 'think': 25, 'is': 26, 'amazing': 27}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gr3dbQfC5VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e048d61-2af4-468d-bae9-f85c5534038f"
      },
      "source": [
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding = 'post')\n",
        "print(padded[0])\n",
        "print(padded.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 6  7  8  3  9  2 10 11 12  0  0  0  0]\n",
            "(4, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZufOahzC6yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dbcae6-8a87-4d2b-a7be-79de2c11a6a0"
      },
      "source": [
        "####################\n",
        "\n",
        "label_tok = Tokenizer()\n",
        "label_tok.fit_on_texts(labels)\n",
        "# label with index\n",
        "lwi  = label_tok.word_index\n",
        "print(lwi)\n",
        "\n",
        "ls = label_tok.texts_to_sequences(labels)\n",
        "print(ls)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tech': 1, 'business': 2, 'sport': 3, 'entertainment': 4, 'politics': 5}\n",
            "[[1], [2], [3], [3], [4], [5], [5], [3], [3], [4], [4], [5], [3], [5], [3], [1], [1], [1], [3], [3], [1], [3], [4], [1], [5], [4], [5], [1], [4], [4], [5], [1], [4], [5], [5], [3], [1], [4], [1], [4], [2], [2], [2], [2], [2], [2], [2], [2], [2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WhJj6FYh2hr"
      },
      "source": [
        "**Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mKRN7dph6Mj"
      },
      "source": [
        "sentences = [\n",
        "    'At DDU, we are Learning ML in semester 7 !!!',\n",
        "    'I love ml subject',\n",
        "    'Prof Brijesh Bhatt and prof. Hariom Pandya are faculties for the Ml subject',\n",
        "    'Do you think ML is amazing?'\n",
        "]\n",
        "\n",
        "##########################################\n",
        "\n",
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love CC subject',\n",
        "    'Do you think BDA and IP are amazing subjects?'\n",
        "]\n",
        "\n",
        "###############################################\n",
        "\n",
        "\n",
        "#Observations: subject/subjects, ML/mL/ml, i/I  prof/Prof.  !!!"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N02qMQ8YnZZw",
        "outputId": "d2d70811-5763-4c75-999a-791e65274bb5"
      },
      "source": [
        "sents = []\n",
        "# print(*sentences)\n",
        "for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    for word in stopwords:\n",
        "      stop_word = \" \"+ word + \" \"\n",
        "      sentence = sentence.replace(stop_word, \" \")\n",
        "      sentence = sentence.replace(\"  \", \" \")\n",
        "    sents.append(sentence)\n",
        "print(*sents,sep=\"\\n\")\n",
        "           \n",
        "sent_tok = Tokenizer(oov_token= \"<OOV>\")\n",
        "sent_tok.fit_on_texts(sents)\n",
        "word_index = sent_tok.word_index\n",
        "print(word_index)\n",
        "\n",
        "\n",
        "\n",
        "seq = sent_tok.texts_to_sequences(sents)\n",
        "padded = pad_sequences(seq, maxlen=15,padding = 'post')\n",
        "print(padded[0])\n",
        "print(padded.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at ddu, learning ml semester 7 !!!\n",
            "i love ml subject\n",
            "prof brijesh bhatt prof. hariom pandya faculties ml subject\n",
            "do think ml amazing?\n",
            "{'<OOV>': 1, 'ml': 2, 'subject': 3, 'prof': 4, 'at': 5, 'ddu': 6, 'learning': 7, 'semester': 8, '7': 9, 'i': 10, 'love': 11, 'brijesh': 12, 'bhatt': 13, 'hariom': 14, 'pandya': 15, 'faculties': 16, 'do': 17, 'think': 18, 'amazing': 19}\n",
            "[5 6 7 2 8 9 0 0 0 0 0 0 0 0 0]\n",
            "(4, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nREBp_zndxF",
        "outputId": "3ea678d7-5383-4c60-81c3-eb40d37c4569"
      },
      "source": [
        "test_data = [\n",
        "    'i really love CC subject',\n",
        "    'Do you think BDA and IP are amazing subjects?'\n",
        "]\n",
        "\n",
        "seq = sent_tok.texts_to_sequences(test_data)\n",
        "padded = pad_sequences(seq, maxlen=15,padding = 'post')\n",
        "\n",
        "print(padded)\n",
        "print(padded[0])\n",
        "print(padded.shape)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  1 11  1  3  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  1 18  1  1  1  1 19  1  0  0  0  0  0  0]]\n",
            "[10  1 11  1  3  0  0  0  0  0  0  0  0  0  0]\n",
            "(2, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Ve00OUng1d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}